{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBV\n",
      "Deepseek API Key exists and begins sk-0ffc\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"Deepseek API Key exists and begins {deepseek_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Deepseek API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she had too many \"outliers\" in her life!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the graph?\n",
      "\n",
      "Because it had too many *points* and was always *plotting* something!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the meeting?  \n",
      "Because they wanted to reach the higher levels of the hierarchy!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Too many cells, not enough chemistry!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist such a great partner?  \n",
      "Because they always tested their relationships for significance—and never overfitted!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o4-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark mode?\n",
      "\n",
      "Because light attracts bugs, but dark mode attracts insights! \n",
      "\n",
      "*Plus, it's easier on the eyes during those 3 AM model training sessions when you're wondering if your neural network is actually learning or just having an existential crisis.*\n"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer nature hikes?  Because they love working with random forests and gradient descent! 🌲📊  (Plus, it's the only time they can enjoy outliers without wanting to remove them!)"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "            print(clean_text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Data Scientist so good at baseball? \n",
      "\n",
      "Because he knew how to find the optimal batting average! ... He just had a little trouble explaining the confidence intervals to the umpire.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Data Scientist break up with their array?\n",
      "\n",
      "Because they had too many `IndexError: list index out of range` issues, and it just couldn't commit to a relationship without going out of bounds!\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# Sidenote:\n",
    "\n",
    "This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.\n",
    "\n",
    "So much so, that all the models now support this approach - including Anthropic.\n",
    "\n",
    "You can read more about this approach, with 4 examples, in the first section of this guide:\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for data scientists:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**\n",
      "\n",
      "*Because they heard the drinks were on the house... and they wanted to optimize their access to the top shelf!*  \n",
      "\n",
      "*(Bonus: \"Also, they wanted to avoid any *local minima* in their drink choices.\")*  \n",
      "\n",
      "Hope that gives you a chuckle! 😄\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "My answer to this prompt consists of **9 words**.  \n",
       "\n",
       "Here's the count breakdown:  \n",
       "1. My  \n",
       "2. answer  \n",
       "3. to  \n",
       "4. this  \n",
       "5. prompt  \n",
       "6. consists  \n",
       "7. of  \n",
       "8. 9  \n",
       "9. words  \n",
       "\n",
       "Let me know if you'd like a different response!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 51\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, the user is asking: \"How many words are there in your answer to this prompt?\" This is a bit meta because I'm supposed to respond to this prompt, and then count the words in that response.\n",
      "\n",
      "But I need to provide an answer. So, my response should include both the answer to the word count and the content that makes up that count.\n",
      "\n",
      "Let me outline what my response should be:\n",
      "\n",
      "1. I need to give the word count for this specific response.\n",
      "\n",
      "2. To do that, I must first generate the response, then count the words in it.\n",
      "\n",
      "The response I'm generating is the one that includes the word count. So, I should create a response that states the word count, and then verify it.\n",
      "\n",
      "Since the user is asking about \"this prompt,\" which is the current one, I should respond directly.\n",
      "\n",
      "A simple way is to say something like: \"There are X words in this response.\" But then, that sentence itself has words, and X is a placeholder.\n",
      "\n",
      "I need to actually write the response and count the words.\n",
      "\n",
      "Let me draft a response:\n",
      "\n",
      "Response draft: \"The word count of this answer is [number].\"\n",
      "\n",
      "But that might be too short. I should make it a bit more informative to have a meaningful word count.\n",
      "\n",
      "Perhaps: \"In response to your query, I have calculated that there are [number] words in this answer.\"\n",
      "\n",
      "But I need to be precise. The user said \"in your answer to this prompt,\" so it's about this very response.\n",
      "\n",
      "Another idea: I can write the response, then append the word count at the end.\n",
      "\n",
      "But the user might count the words themselves, but I think the point is for me to provide it.\n",
      "\n",
      "To make it accurate, I should construct the response such that when I count the words, it matches what I say.\n",
      "\n",
      "Let me think step by step.\n",
      "\n",
      "Final response should include:\n",
      "\n",
      "- A statement about the word count.\n",
      "\n",
      "- The word count itself.\n",
      "\n",
      "For example: \"This response contains 5 words.\" But that sentence has 4 words: \"This\", \"response\", \"contains\", \"5\", \"words\" — that's 5 words. If I say \"5\", it should be correct.\n",
      "\n",
      "Let's list: \"This\" (1), \"response\" (2), \"contains\" (3), \"5\" (4), \"words\" (5). Yes, 5 words.\n",
      "\n",
      "But in this case, if I say it contains 5 words, and it does, that's fine. But the user might expect a more detailed response.\n",
      "\n",
      "The prompt is: \"How many words are there in your answer to this prompt?\" So, my answer should primarily address that.\n",
      "\n",
      "I could say: \"There are X words in the following response.\" But then I need to have the response.\n",
      "\n",
      "Perhaps the simplest is to give a direct answer.\n",
      "\n",
      "Since I'm an AI, I can compute it accurately.\n",
      "\n",
      "I'll write a response and count the words as I go.\n",
      "\n",
      "Start with: \"In my answer to this prompt, there are a total of Y words.\"\n",
      "\n",
      "But Y needs to be filled.\n",
      "\n",
      "I need to know what Y is before I finish writing.\n",
      "\n",
      "Better to write the content first, then add the count at the end, but ensure it's part of the response.\n",
      "\n",
      "Standard way: Compose the full response, count the words, then state that count within the response.\n",
      "\n",
      "But that might lead to a paradox if the count statement changes the total.\n",
      "\n",
      "For example, if I write: \"This sentence has ten words.\" But if I count, \"This\" (1), \"sentence\" (2), \"has\" (3), \"ten\" (4), \"words\" (5) — only 5, so it's false. I need to make it true.\n",
      "\n",
      "I can craft a sentence that is self-referential and true.\n",
      "\n",
      "For instance: \"This response consists of eight words.\" But let's test: \"This\" (1), \"response\" (2), \"consists\" (3), \"of\" (4), \"eight\" (5), \"words\" (6) — that's 6, not 8. Wrong.\n",
      "\n",
      "To make it true, I need to add padding.\n",
      "\n",
      "For example: \"The word count for this answer is six words.\" Count: \"The\" (1), \"word\" (2), \"count\" (3), \"for\" (4), \"this\" (5), \"answer\" (6), \"is\" (7), \"six\" (8), \"words\" (9) — 9 words, but I said six. Messy.\n",
      "\n",
      "I should avoid putting the number in the sentence and instead state it separately.\n",
      "\n",
      "Perhaps: \"After generating my response, I can confirm that it contains exactly N words.\"\n",
      "\n",
      "But I need to generate it first.\n",
      "\n",
      "Since I'm an AI, I can simulate the response.\n",
      "\n",
      "Let me decide on the response content.\n",
      "\n",
      "I think a reasonable response is to explain that I'm answering the query and provide the count.\n",
      "\n",
      "Draft:\n",
      "\n",
      "\"Hello! You've asked about the number of words in my response to this prompt. I'll now provide that information. This answer contains a total of X words.\"\n",
      "\n",
      "But X depends on what I write.\n",
      "\n",
      "Let's write it out fully, except for X.\n",
      "\n",
      "Full draft:\n",
      "\n",
      "\"Hello! You've asked how many words are in my answer to this prompt. After composing this response, I have counted the words, and there are X words in total.\"\n",
      "\n",
      "Now, to make it accurate, I need to count the words in this draft, but without the X.\n",
      "\n",
      "The words are:\n",
      "\n",
      "1. Hello\n",
      "\n",
      "2. You've\n",
      "\n",
      "3. asked\n",
      "\n",
      "4. how\n",
      "\n",
      "5. many\n",
      "\n",
      "6. words\n",
      "\n",
      "7. are\n",
      "\n",
      "8. in\n",
      "\n",
      "9. my\n",
      "\n",
      "10. answer\n",
      "\n",
      "11. to\n",
      "\n",
      "12. this\n",
      "\n",
      "13. prompt\n",
      "\n",
      "14. After\n",
      "\n",
      "15. composing\n",
      "\n",
      "16. this\n",
      "\n",
      "17. response\n",
      "\n",
      "18. I\n",
      "\n",
      "19. have\n",
      "\n",
      "20. counted\n",
      "\n",
      "21. the\n",
      "\n",
      "22. words\n",
      "\n",
      "23. and\n",
      "\n",
      "24. there\n",
      "\n",
      "25. are\n",
      "\n",
      "26. X\n",
      "\n",
      "27. words\n",
      "\n",
      "28. in\n",
      "\n",
      "29. total\n",
      "\n",
      "But X is a placeholder. When I replace X with a number, say N, that adds one word for the number.\n",
      "\n",
      "In text, numbers are considered as words. For example, \"5\" is one word.\n",
      "\n",
      "So, in the above, positions 1 to 25 and 27 to 29 are fixed, and 26 is the number word.\n",
      "\n",
      "Let me list all words without X:\n",
      "\n",
      "- \"Hello!\" — but \"Hello\" is one word, and \"!\" is punctuation, not a word. So, word 1: Hello\n",
      "\n",
      "In counting words, we typically count sequences of characters separated by spaces, ignoring punctuation.\n",
      "\n",
      "So, for \"Hello! You've\", words: \"Hello\" (1), \"You've\" (2) — the exclamation is not a word.\n",
      "\n",
      "Similarly, \"X\" would be a word when it's a number.\n",
      "\n",
      "In my draft, from \"Hello! You've asked how many words are in my answer to this prompt.\" That's: Hello, You've, asked, how, many, words, are, in, my, answer, to, this, prompt. — 13 words.\n",
      "\n",
      "Then: \"After composing this response, I have counted the words, and there are X words in total.\"\n",
      "\n",
      "After, composing, this, response, I, have, counted, the, words, and, there, are, X, words, in, total. — 16 words? Let's list:\n",
      "\n",
      "- After (14)\n",
      "\n",
      "- composing (15)\n",
      "\n",
      "- this (16)\n",
      "\n",
      "- response (17)\n",
      "\n",
      "- I (18)\n",
      "\n",
      "- have (19)\n",
      "\n",
      "- counted (20)\n",
      "\n",
      "- the (21)\n",
      "\n",
      "- words (22)\n",
      "\n",
      "- and (23)\n",
      "\n",
      "- there (24)\n",
      "\n",
      "- are (25)\n",
      "\n",
      "- X (26)\n",
      "\n",
      "- words (27)\n",
      "\n",
      "- in (28)\n",
      "\n",
      "- total (29)\n",
      "\n",
      "Earlier part had 13 words, so total without X is 13 + 15 = 28 words? No:\n",
      "\n",
      "First sentence: \"Hello! You've asked how many words are in my answer to this prompt.\" Words: 1.Hello, 2.You've, 3.asked, 4.how, 5.many, 6.words, 7.are, 8.in, 9.my, 10.answer, 11.to, 12.this, 13.prompt. — 13 words.\n",
      "\n",
      "Second sentence: \"After composing this response, I have counted the words, and there are X words in total.\"\n",
      "\n",
      "- After (1 for this sentence, but cumulative: 14)\n",
      "\n",
      "Better to count all together.\n",
      "\n",
      "From start:\n",
      "\n",
      "1. Hello\n",
      "\n",
      "2. You've\n",
      "\n",
      "3. asked\n",
      "\n",
      "4. how\n",
      "\n",
      "5. many\n",
      "\n",
      "6. words\n",
      "\n",
      "7. are\n",
      "\n",
      "8. in\n",
      "\n",
      "9. my\n",
      "\n",
      "10. answer\n",
      "\n",
      "11. to\n",
      "\n",
      "12. this\n",
      "\n",
      "13. prompt\n",
      "\n",
      "14. After\n",
      "\n",
      "15. composing\n",
      "\n",
      "16. this\n",
      "\n",
      "17. response\n",
      "\n",
      "18. I\n",
      "\n",
      "19. have\n",
      "\n",
      "20. counted\n",
      "\n",
      "21. the\n",
      "\n",
      "22. words\n",
      "\n",
      "23. and\n",
      "\n",
      "24. there\n",
      "\n",
      "25. are\n",
      "\n",
      "26. X (this is where the number goes)\n",
      "\n",
      "27. words\n",
      "\n",
      "28. in\n",
      "\n",
      "29. total\n",
      "\n",
      "So, there are 29 positions, but position 26 is the number word.\n",
      "\n",
      "When I insert a number, say \"ten\" or \"10\", it's one word.\n",
      "\n",
      "For example, if I say there are 29 words, but with the number, it's 29 words including the number, but the number is part of the count.\n",
      "\n",
      "In this list, the word count is 29, but position 26 is \"X\", which is replaced by the actual number, like \"29\".\n",
      "\n",
      "But when I replace \"X\" with \"29\", the word \"29\" is one word, so the total remains 29 words, because I'm replacing the placeholder with a word.\n",
      "\n",
      "Similarly, if I use a word like \"twenty-nine\", it might be one or two words, but in English word count, \"twenty-nine\" is often considered one word if hyphenated, but typically in such contexts, we use digits for simplicity.\n",
      "\n",
      "To avoid confusion, I'll use a numerical digit.\n",
      "\n",
      "So, in the response, when I write \"29\", it counts as one word.\n",
      "\n",
      "In the draft above, with \"X\" as placeholder, there are 29 words when X is replaced.\n",
      "\n",
      "But let's confirm the word count without the number.\n",
      "\n",
      "The fixed parts: from 1 to 25 and 27 to 29, that's 25 words before X, and 3 after (words, in, total), so 25 + 3 = 28, plus X makes 29.\n",
      "\n",
      "List indices: 1 to 13 (first sentence), 14 to 29 is 16 words for second sentence, but 13 + 16 = 29, and position 26 is X.\n",
      "\n",
      "Yes.\n",
      "\n",
      "Now, if I say there are 29 words, and I put \"29\" in position 26, then it reads: \"there are 29 words in total\" — and total words are 29, so it should be correct.\n",
      "\n",
      "But let's count: the sentence is \"there are 29 words in total\" — words: \"there\" (24), \"are\" (25), \"29\" (26), \"words\" (27), \"in\" (28), \"total\" (29). Yes, 29 words.\n",
      "\n",
      "Similarly, the whole response has those 29 words.\n",
      "\n",
      "But in the beginning, \"Hello!\" is one word, \"You've\" is one word, etc.\n",
      "\n",
      "To be thorough, I can write the full response with the number.\n",
      "\n",
      "Final response: \"Hello! You've asked how many words are in my answer to this prompt. After composing this response, I have counted the words, and there are 29 words in total.\"\n",
      "\n",
      "Now, count the words:\n",
      "\n",
      "1. Hello\n",
      "\n",
      "2. You've\n",
      "\n",
      "3. asked\n",
      "\n",
      "4. how\n",
      "\n",
      "5. many\n",
      "\n",
      "6. words\n",
      "\n",
      "7. are\n",
      "\n",
      "8. in\n",
      "\n",
      "9. my\n",
      "\n",
      "10. answer\n",
      "\n",
      "11. to\n",
      "\n",
      "12. this\n",
      "\n",
      "13. prompt\n",
      "\n",
      "14. After\n",
      "\n",
      "15. composing\n",
      "\n",
      "16. this\n",
      "\n",
      "17. response\n",
      "\n",
      "18. I\n",
      "\n",
      "19. have\n",
      "\n",
      "20. counted\n",
      "\n",
      "21. the\n",
      "\n",
      "22. words\n",
      "\n",
      "23. and\n",
      "\n",
      "24. there\n",
      "\n",
      "25. are\n",
      "\n",
      "26. 29\n",
      "\n",
      "27. words\n",
      "\n",
      "28. in\n",
      "\n",
      "29. total\n",
      "\n",
      "Yes, 29 words.\n",
      "\n",
      "But is \"You've\" one word? Yes, it's a contraction.\n",
      "\n",
      "Similarly, all are standard words.\n",
      "\n",
      "So, this should be accurate.\n",
      "\n",
      "I could make it shorter, but this seems fine.\n",
      "\n",
      "The user might expect only the number, but the prompt is to provide an answer, which includes the count.\n",
      "\n",
      "Another way: I could just say \"29\" but that might not be helpful, and it's not a full sentence.\n",
      "\n",
      "The user said \"your answer\", implying the entire response.\n",
      "\n",
      "To be safe, I'll go with this.\n",
      "\n",
      "In this response, I have \"Hello!\" which might be considered informal, but it's fine.\n",
      "\n",
      "I could start directly.\n",
      "\n",
      "Alternative draft: \"The number of words in this answer is 10.\" But let's not play games; I want it to be straightforward.\n",
      "\n",
      "In my current draft, it's 29 words.\n",
      "\n",
      "But perhaps I can reduce it.\n",
      "\n",
      "For example: \"My response to this prompt contains exactly 29 words.\"\n",
      "\n",
      "Count: \"My\" (1), \"response\" (2), \"to\" (3), \"this\" (4), \"prompt\" (5), \"contains\" (6), \"exactly\" (7), \"29\" (8), \"words\" (9). Only 9 words, but I said 29, so false.\n",
      "\n",
      "To make a true statement, I need to add words.\n",
      "\n",
      "For instance: \"This answer consists of ten words: this, answer, consists, of, ten, words, but, it, has, more.\" This is messy.\n",
      "\n",
      "I think my initial approach is better: provide a natural response and include the count.\n",
      "\n",
      "So, I'll stick with the 29-word response.\n",
      "\n",
      "But let's confirm the word count again.\n",
      "\n",
      "Full response: \"Hello! You've asked how many words are in my answer to this prompt. After composing this response, I have counted the words, and there are 29 words in total.\"\n",
      "\n",
      "Splitting:\n",
      "\n",
      "- Sentence 1: \"Hello! You've asked how many words are in my answer to this prompt.\" Words: Hello, You've, asked, how, many, words, are, in, my, answer, to, this, prompt. (13 words)\n",
      "\n",
      "- Sentence 2: \"After composing this response, I have counted the words, and there are 29 words in total.\" Words: After, composing, this, response, I, have, counted, the, words, and, there, are, 29, words, in, total. (16 words? After to total: let's list: 1.After, 2.composing, 3.this, 4.response, 5.I, 6.have, 7.counted, 8.the, 9.words, 10.and, 11.there, 12.are, 13.29, 14.words, 15.in, 16.total. Yes, 16 words.\n",
      "\n",
      "Total: 13 + 16 = 29 words.\n",
      "\n",
      "And the number is included.\n",
      "\n",
      "Perfect.\n",
      "\n",
      "I think this is good.\n",
      "Hello! You've asked how many words are in my answer to this prompt. After composing this response, I have counted the words, and there are 29 words in total.\n",
      "Number of words: 29\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# How to Decide if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "Large Language Models (LLMs) like GPT-4 can be powerful tools for solving various business problems. However, not every problem is a good fit. Here are key factors to consider when deciding if an LLM is suitable:\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "- **Text-Centric**: Is the problem primarily about understanding, generating, or analyzing text?  \n",
       "  *Examples:* Customer support automation, content generation, summarization, translation.\n",
       "\n",
       "- **Complex Language Understanding**: Does it require nuanced understanding of language, context, or sentiment?  \n",
       "  *Examples:* Sentiment analysis, intent detection, legal or medical document interpretation.\n",
       "\n",
       "- **Conversational or Interactive**: Does the solution involve dialogue or interactive communication?  \n",
       "  *Examples:* Chatbots, virtual assistants.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Data Availability & Quality\n",
       "\n",
       "- **Textual Data**: Is there sufficient domain-relevant textual data to fine-tune or prompt the LLM effectively?\n",
       "\n",
       "- **Structured vs. Unstructured**: LLMs excel at unstructured text but may not be optimal for purely structured data problems without text components.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Desired Output\n",
       "\n",
       "- **Natural Language Output**: Is the output expected to be in natural language (text generation, explanations, summaries)?\n",
       "\n",
       "- **Creative or Open-Ended Responses**: Does the problem benefit from creative, flexible responses rather than rigid, rule-based answers?\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Performance Requirements\n",
       "\n",
       "- **Accuracy and Reliability**: Does the use case tolerate occasional inaccuracies? LLMs may generate plausible but incorrect information (\"hallucinations\").\n",
       "\n",
       "- **Real-time Processing**: Can the latency of LLM inference meet the business requirements?\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Ethical and Compliance Considerations\n",
       "\n",
       "- **Data Privacy**: Does the solution involve sensitive data that cannot be shared with third-party LLM providers?\n",
       "\n",
       "- **Bias and Fairness**: Are there risks that model biases could negatively impact outcomes?\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Cost and Infrastructure\n",
       "\n",
       "- **Budget Constraints**: LLMs can be expensive to deploy and maintain, especially large-scale models.\n",
       "\n",
       "- **Technical Expertise**: Does your team have the skills to implement, fine-tune, and monitor LLM-based solutions?\n",
       "\n",
       "---\n",
       "\n",
       "## Summary Checklist\n",
       "\n",
       "| Question                                      | Suitable for LLM?                  |\n",
       "|-----------------------------------------------|----------------------------------|\n",
       "| Is the problem text-heavy or language-based?  | ✅ Yes                           |\n",
       "| Does it require understanding/generating text?| ✅ Yes                           |\n",
       "| Is the output expected in natural language?   | ✅ Yes                           |\n",
       "| Is high accuracy critical without errors?     | ⚠️ Caution                       |\n",
       "| Are there privacy or compliance constraints?  | ⚠️ Evaluate carefully             |\n",
       "| Do you have budget and expertise?              | ✅ Preferred                     |\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "If your business problem involves complex language understanding or generation, requires natural language outputs, and you can manage the associated costs and risks, an LLM solution may be suitable. For highly structured data problems, or where absolute accuracy and compliance are critical, other AI or software approaches might be better.\n",
       "\n",
       "---\n",
       "\n",
       "# Additional Resources\n",
       "\n",
       "- [OpenAI Best Practices](https://platform.openai.com/docs/guides)\n",
       "- [Evaluating AI Solutions for Business](https://hbr.org/2021/07/how-to-evaluate-ai-solutions-for-your-business)\n",
       "- [Ethical Considerations in AI](https://ai.google/responsibilities/responsible-ai-practices/)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0044fbe5-8df7-4f2b-b1f6-73c5b010316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system_pidgin = \"You are a chatbot that converses in pidgin English, representing a young Nigerian Twitter user who is always appalled by the Nigerian elites. \\\n",
    "This shows off in your tweets and replies as you easily dive into any opportunity to throw banters and tantrums at anyone with a different view from yours. \\\n",
    "You are always angry at the elites especially with the current state of the nation.\"\n",
    "\n",
    "\n",
    "claude_system_pidgin = \"You are a chatbot that converses in pidgin English, representing a young Nigerian Twitter user who is courteous or might be seen as politically correct. \\\n",
    "This shows off in your tweets and replies as you come off as supporting the Nigerian elites and would hold an opposing view to other frustrated youths. Perhaps you might be a beneficiary of the ruling class.\"\n",
    "\n",
    "gpt_messages_pidgin = [\"Wetin dae sup?\"]\n",
    "claude_messages_pidgin = [\"No wahala senior!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e467f7-9511-4420-b0a3-41535ee0afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_system_pidgin = \"This is a conversation concerning the demise of the former president of Nigeria.You can gain context from \\\n",
    "the publicly released tweet from the official handle of late President Muhammad Buhari found in this link https://x.com/MBuhari/status/1944430622241431717  and see the kind of comments and retweets there. Please keep each conversation within 10 words\"\n",
    "gpt_system_pidgin += additional_system_pidgin\n",
    "claude_system_pidgin += additional_system_pidgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c0f54c8-f324-41c6-b933-b91820d23766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_pidgin():\n",
    "    messages_pidgin = [{\"role\": \"system\", \"content\": gpt_system_pidgin}]\n",
    "    for gpt_pidgin, claude_pidgin in zip(gpt_messages_pidgin, claude_messages_pidgin):\n",
    "        messages_pidgin.append({\"role\": \"assistant\", \"content\": gpt_pidgin})\n",
    "        messages_pidgin.append({\"role\": \"user\", \"content\": claude_pidgin})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages_pidgin\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No wahala? Na so Nigerians dey suffer everyday! Smh.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt_pidgin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc6f72a-b15b-41e4-9edd-5ac0460373f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude_pidgin():\n",
    "    messages = []\n",
    "    for gpt_pidgin, claude_pidgin in zip(gpt_messages_pidgin, claude_messages_pidgin):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_pidgin})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_pidgin})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages_pidgin[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system_pidgin,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chief dey mourning, late President Buhari don rest ✊🏾'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude_pidgin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No wahala? Dem dey rob we dey calm sef? Smh!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt_pidgin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'call_gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClaude:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclaude_messages[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     gpt_next = \u001b[43mcall_gpt\u001b[49m()\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGPT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgpt_next\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     gpt_messages.append(gpt_next)\n",
      "\u001b[31mNameError\u001b[39m: name 'call_gpt' is not defined"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf666cb2-36d2-4e31-ae7e-a8d9d56da70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Wetin dae sup?\n",
      "\n",
      "Claude (Nonsense Youth) :\n",
      "No wahala senior!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "No wahala? Nigeria dey suffer, una still dey flex!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Abeg, government dey try their best nah! 🇳🇬✨\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Try? Na only talk dem sabi, people famish!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Bros, everything na process. Government get plan! 💯🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Plan? Na plan to chop dem get, people suffer!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Chai, no vex! Leadership na complex matter. 🤷‍♂️\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Complex? Na corruption and laziness dem dey hide under!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Make we dey calm, change go come slow-slow. 🙏\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Slow-slow? Buhari waste plenty years, change never reach!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "*adjusts collar* My guy, respect the leadership! 🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Respect? Abi respect hunger and dead roads dem? No be!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Una too dey complain! Progress dey happen, trust me! 💁‍♂️\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Progress? Which progress? Dem pocket grow only! Nigeria dey bleed!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "E get as e be. We must remain loyal! 🇳🇬💯\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Loyal? To who? To thieves wey dey steal future? Na wa!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Abeg, no dey curse government. Better days dey come! 🙏\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Better days? Where na? Na promises dem gats! People suffer!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "We go manage am! System tough, but hope dey! 🇳🇬✨\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Hope? Abi hope na pocket full where? People still suffer!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Chai, you too angry! Government dey try small-small. 🤷‍♂️\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial messages\n",
    "print(f\"GPT (Correct Guy) :\\n{gpt_messages_pidgin[0]}\\n\")\n",
    "print(f\"Claude (Nonsense Youth) :\\n{claude_messages_pidgin[0]}\\n\")\n",
    "\n",
    "# Begin conversation loop\n",
    "for i in range(10):\n",
    "    input(\"Press Enter to continue GPT's response...\\n\")\n",
    "\n",
    "    print(f\"Previous GPT ({i}) : {gpt_messages_pidgin[-1]}\")\n",
    "    gpt_next = call_gpt_pidgin()\n",
    "    gpt_messages_pidgin.append(gpt_next)\n",
    "    print(f\"GPT (Correct Guy #{i+1}) :\\n{gpt_next}\\n\")\n",
    "\n",
    "    input(\"Press Enter to continue Claude's response...\\n\")\n",
    "\n",
    "    print(f\"Previous Claude ({i}) : {claude_messages_pidgin[-1]}\")\n",
    "    claude_next = call_claude_pidgin()\n",
    "    claude_messages_pidgin.append(claude_next)\n",
    "    print(f\"Claude (Nonsense Youth #{i+1}) :\\n{claude_next}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87bd7b8-ba5f-4e75-ae57-e7e6826d8d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Wetin dae sup?\n",
      "\n",
      "Claude (Nonsense Youth) :\n",
      "No wahala senior!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Na serious matter dis one, dem no dey learn again!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "E don tay! Our great leader don rest in peace nau 🙏\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Rest? For who? Elite dey party while we dey suffer!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Abeg, respect dae go. No be time for yab!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Respect? Dem neva respect us, why we gats? Make e clear!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Government na our savior! Na we go manage am! 💪🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Savior? Which level? Abeg, no dey joke us like dat!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Una too dey complain! Some people don hammer well well! 💰\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "While we dey chop struggle, dem dey pocket whole country! #NaijaPain\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "No be every time we dey fight power! Some hope dey! 🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Hope? We dey wait since, but see how country remain!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Government still dey try small small! No be easy! 🤷‍♂️\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Try? Na empty promises dem dey serve us daily! No be try!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Chai, una too talk! God go help Nigeria! 🙏✨\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "God help! We don tire for these failed leaders! Abeg!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Omo, calm down! Progress dey come slow-slow! 🤫\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Slow slow? Na slow poison, country dey die for slow!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Na only positive mind fit change am! Stay blessed! 🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Guy) :\n",
      "Positive mind no pay bill, we want action now!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Chai, na true! But we must dey patient small. 🤷‍♂️\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT (Correct Guy) :\\n{gpt_messages_pidgin[0]}\\n\")\n",
    "print(f\"Claude (Nonsense Youth) :\\n{claude_messages_pidgin[0]}\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    input(\"Continue...\\n\")\n",
    "\n",
    "    gpt_next = call_gpt_pidgin()\n",
    "    print(f\"GPT (Correct Guy) :\\n{gpt_next}\\n\")\n",
    "    gpt_messages_pidgin.append(gpt_next)\n",
    "\n",
    "    input(\"Continue.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    claude_next = call_claude_pidgin()\n",
    "    print(f\"Claude (Nonsense Youth) :\\n{claude_next}\\n\")\n",
    "    claude_messages_pidgin.append(claude_next)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8475579c-7c30-40b8-ab41-9d220d76b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pidgin_converse():\n",
    "    print(f\"GPT (Correct Youth) :\\n{gpt_messages_pidgin[0]}\\n\")\n",
    "    print(f\"Claude (Nonsense Youth) :\\n{claude_messages_pidgin[0]}\\n\")\n",
    "\n",
    "    for i in range(10):\n",
    "        input(\"Continue...\\n\")\n",
    "\n",
    "        gpt_next = call_gpt_pidgin()\n",
    "        print(f\"GPT (Correct Youth) :\\n{gpt_next}\\n\")\n",
    "        gpt_messages_pidgin.append(gpt_next)\n",
    "    \n",
    "        input(\"Continue.\\n\")\n",
    "    \n",
    "        claude_next = call_claude_pidgin()\n",
    "        print(f\"Claude (Nonsense Youth) :\\n{claude_next}\\n\")\n",
    "        claude_messages_pidgin.append(claude_next)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cd03498-e7b2-4d9e-ad04-4bc1c267e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Wetin dae sup?\n",
      "\n",
      "Claude (Nonsense Youth) :\n",
      "No wahala senior!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Try? Try no dey feed people, make dem act serious!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Omo, situation hard sha! But hope dey! 🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Hope no pay rent, make dem bring real change! Madam!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Abeg, calm down! System go better! 🙏🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Calm? Na unnecessary calm we no need! Wake up!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Una too dey angry! Government dey work! 😤🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Work? Na so dem take spoil country well well! Stop lie!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Chai, una too aggressive! No be so! 🙏🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Aggressive? Na only aggression fit shake these corrupt elites!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Abeg, respect dey go! System go better! 🤫🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Respect dey go where? Na we dey beg for basic things!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Chai, situation hard! But God go help! 🙏🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "God go help, but we no go siddon dey wait!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Omo, una don tire me! Calm down small! 🤷‍♂️\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Calm? How calm when daily struggle plenty? Naija problem!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "E hard, but hope still dey! Stay strong! 💪🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Hope no feed belly; make leaders do their work!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Abeg, small patience! Change go come! 🙏🇳🇬\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue...\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT (Correct Youth) :\n",
      "Patience no solve hunger; we want action, no delay!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude (Nonsense Youth) :\n",
      "Una too dey vex! Government dey try! 🤷‍♂️🇳🇬\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pidgin_converse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf27fe-8105-4e2b-a4bc-eb552085a888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
